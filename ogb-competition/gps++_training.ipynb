{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for Molecular Property Prediction using GPS++ on IPUs (OGB-LSC)\n",
    "==========\n",
    "\n",
    "This notebook demonstrates how to run training for the GPS++ model architecture we used for our OGB-LSC PCQM4Mv2 submission. We will discuss GPS++ in this notebook but for more details on GPS++ see [GPS++: Reviving the Art of Message Passing for Molecular Property Prediction](https://arxiv.org/abs/2302.02947).\n",
    "\n",
    "The challenge is to predict the HOMO-LUMO gap [[1]](https://en.wikipedia.org/wiki/HOMO_and_LUMO) of \n",
    "organic molecules, a useful property correlated to the stability of a compound. \n",
    "Typically, such values are obtained from density functional theory (DFT) using high-performance compute (HPC) methods. These\n",
    "simulations are expensive and time consuming to run, and the objective of the challenge is to \n",
    "use machine learning to approximate the simulation and obtain results in a fraction of the time. \n",
    "\n",
    "We show here a smaller model of 11 million parameters, as opposed to the 44 million used for the top-3 result, as this will train in approximately four hours and uses only four IPUs.\n",
    "\n",
    "In the process of doing this we will see some of the additional features we generate from the original dataset and feed into the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running on Paperspace\n",
    "\n",
    "> Currently, this notebook requires a POD16 machine type to run successfully.\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/gradient-ai/Graphcore-Tensorflow2/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements:**\n",
    "* Python packages installed with `pip install -r requirements.txt`\n",
    "\n",
    "**Troubleshooting:**\n",
    "\n",
    "* If you see an `Unexpected error` when starting the machine, refresh the page and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples_utils import notebook_logging\n",
    "%load_ext gc_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example requires building a few things:\n",
    "\n",
    "- An optimised method to get the path lengths of a graph\n",
    "- IPU-optimised grouped gather/scatter operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "code_directory = os.getenv(\"OGB_SUBMISSION_CODE\", \".\")\n",
    "! cd {code_directory} && make -C data_utils/feature_generation\n",
    "! cd {code_directory} && make -C static_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will throw an error if packages have not had time to load, if this cell throws an error restart the kernel\n",
    "\n",
    "%matplotlib inline\n",
    "# Need notebook utils as first import as it modifies the path\n",
    "import notebook_utils\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from matplotlib import rcParams\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "\n",
    "from argparser import parse_dict\n",
    "from data_utils.load_dataset import load_raw_dataset\n",
    "from data_utils.preprocess_dataset import preprocess_dataset\n",
    "from notebook_utils import predict, train\n",
    "import os\n",
    "from ogb.lsc import PCQM4Mv2Evaluator\n",
    "from inference import format_predictions\n",
    "\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "rcParams[\"xtick.labelsize\"] = 10\n",
    "rcParams[\"ytick.labelsize\"] = 10\n",
    "rcParams[\"axes.labelsize\"] = 14\n",
    "IPythonConsole.drawOptions.addAtomIndices = False\n",
    "IPythonConsole.drawOptions.addStereoAnnotation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights & Biases logging\n",
    "\n",
    "We use `wandb` to log training metrics, and manage training runs. \n",
    "This notebook will default to using wandb offline. To use their online tracking uncomment the following two lines, and remove the `!wandb offline` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these two lines if you want to log to wandb online\n",
    "# !wandb login $YOUR_WANDB_API_KEY\n",
    "# !wandb online\n",
    "\n",
    "# If running without a wandb login leave this line, remove if you want to log online\n",
    "!wandb offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Paperspace we will run some additional configuration steps below. If you aren't running on Paperspace, ensure you have the following environment variables set: `DATASETS_DIR` — location of the dataset, `CHECKPOINT_DIR` — location of any checkpoints, and `POPLAR_EXECUTABLE_CACHE_DIR` — location of any Poplar executable caches. Or you can update the paths manually in the two cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = os.getenv(\"OGB_CHECKPOINT_DIR\", \".\")\n",
    "dataset_directory = os.getenv(\"OGB_DATASETS_DIR\", \".\")\n",
    "code_directory = Path(os.getenv(\"OGB_SUBMISSION_CODE\", \".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set a few things to enable us to use the executable caches, saving us from recompiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \".\")\n",
    "os.environ[\"TF_POPLAR_FLAGS\"] = f\"--executable_cache_path='{executable_cache_dir}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a configuration\n",
    "\n",
    "For this example, we will use the `GPS_PCQ_4gps_11M.yaml` configuration in the `configs` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "model_name = \"GPS_4layer\"\n",
    "\n",
    "# Set configs\n",
    "model_dict = {\"GPS_4layer\": \"GPS_PCQ_4gps_11M.yaml\"}\n",
    "cfg_path = code_directory / \"configs\" / model_dict[model_name]\n",
    "cfg_yaml = yaml.safe_load(cfg_path.read_text())\n",
    "cfg = parse_dict(cfg_yaml)\n",
    "\n",
    "# Set the checkpoint path for the corresponding config\n",
    "sub_directory = model_dict[model_name].split(\".\")[0]\n",
    "checkpoint_path = Path(checkpoint_directory).joinpath(\n",
    "    f\"checkpoints/{sub_directory}/model-FINAL\"\n",
    ")\n",
    "\n",
    "# Turn off dataset caching for this notebook\n",
    "cfg.dataset.save_to_cache = False\n",
    "cfg.dataset.load_from_cache = True\n",
    "cfg.dataset.cache_path = dataset_directory\n",
    "cfg.dataset.split_path = Path(dataset_directory).joinpath(\"pcqm4mv2-cross_val_splits\")\n",
    "\n",
    "# wandb setup from configuration file\n",
    "if cfg.wandb:\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"notebook_training.ipynb\"\n",
    "    wandb.init(entity=cfg.wandb_entity, project=cfg.wandb_project, config=cfg.as_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the HOMO-LUMO gap of molecules in the PCQM4Mv2 Dataset\n",
    "\n",
    "First, we need to load the raw dataset. This contains SMILE strings [[2]](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system)\n",
    "and the HOMO-LUMO gap calculated with DFT. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset: {cfg.dataset.dataset_name}\")\n",
    "split_mode = \"original\"\n",
    "graph_data = load_raw_dataset(cfg.dataset.dataset_name, dataset_directory, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_df = graph_data.dataset.load_smile_strings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing the dataset let's look at some example molecules. \\\n",
    "We take a random index of the dataset, get the smile string, and plot the molecule.\n",
    "\n",
    "\n",
    "**Feel free to run the following block multiple times to see different example molecules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random index from the train dataset split\n",
    "r_idx = np.random.choice(graph_data.dataset.get_idx_split()[\"train\"], 8)\n",
    "# Extract SMILES, RDKit molecular objects, ground_truth labels, and predictions for these molecules\n",
    "Smiles = [smiles_df[0][r] for r in r_idx]\n",
    "GT = [smiles_df[1][r] for r in r_idx]\n",
    "Mols = [Chem.MolFromSmiles(r) for r in Smiles]\n",
    "\n",
    "# Create labels\n",
    "labelList = [f\"HOMO-LUMO: \" + str(\"%.3f\" % gt) + \" eV\" for gt in zip(GT)]\n",
    "# Display molecules with labels\n",
    "Draw.MolsToGridImage(\n",
    "    Mols,\n",
    "    molsPerRow=4,\n",
    "    legends=[label for label in labelList],\n",
    "    subImgSize=(250, 250),\n",
    "    useSVG=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to preprocess the dataset. This is time consuming, so instead we will load an already preprocessed dataset from the cache.\n",
    "\n",
    "Note that if you want to play around with changing the dataset features this will take longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the cache this step should take ~ 5 minutes\n",
    "graph_preprocessed = preprocess_dataset(dataset=graph_data, options=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPS++ model is a hybrid message passing neural network and transformer, which builds on the previous work of the general, powerful, scalable (GPS) framework [3](https://arxiv.org/abs/2205.12454).\n",
    "\n",
    "The key advantage of this architecture is that by combining the large and expressive message-passing module with a biased self-attention layer local inductive biases can be exploited while still allowing efficient global communication. \n",
    "\n",
    "Additionally, we incorporate grouped input masking, and use the available 3D information as an auxiliary denoising objective during training. \n",
    "\n",
    "The GPS layers compose the majority (> 99%) of the model parameters. Below is a diagram showing how the MPNN and self-attention modules interact with each other.\n",
    "\n",
    "For further details on the architecture and training process look at our paper [GPS++: Reviving the Art of Message Passing for Molecular Property Prediction](https://arxiv.org/abs/2302.02947)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the main GPS++ processing block showing global, edge and node features, as well as attention biases passing through each GPS layer.\n",
    "\n",
    "![GPS++ processing block](pcqm4mv2_submission/OGB_paper_diagram.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(code_directory / \"OGB_paper_diagram.png\", width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we are ready to run on the IPU. We have wrapped this functionality in a single function for simplicity. We encourage you to check the contents of this function in `notebook_utils.py`.\n",
    "\n",
    "Some key details here are:\n",
    "* The main regression loss is measured as the mean absolute error (L1 loss) between predicted and target HOMO-LUMO gaps\n",
    "* With the original dataset split we loop through the ~ 3.3M molecules in the training dataset each epoch\n",
    "* The 11 million parameter model is pipelined over four IPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now. finally, let's run the training of our GPS++ model.\n",
    "\n",
    "In the interest of time, we will set the number of training epochs to 10, which takes approximately 40 minutes. Feel free to train for more epochs. As a guide, training for the full 100 epochs takes approximately 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.epochs = 10\n",
    "checkpoint_paths = train(graph_preprocessed, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(NOTE: The notebook has been provided with the training run for 10 epochs only. If you want to run the full training this will take about 4 hours.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on validation dataset split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get predictions on the validation dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, ground_truth = predict(graph_data, checkpoint_paths[\"FINAL\"], \"valid\", cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the mean, variance and histogram of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = predictions.astype(float).mean()\n",
    "std = predictions.astype(float).var()\n",
    "\n",
    "bins = plt.hist(predictions, 50, alpha=0.7, label=\"Predictions\")[1]\n",
    "plt.hist(ground_truth, bins, alpha=0.5, label=\"Ground truth\")\n",
    "plt.xlabel(\"HOMO-LUMO Gap (eV)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Histogram of HOMO-LUMO gap predictions\")\n",
    "plt.text(8, 8200, f\"mean: {mean:.2f}, std: {std:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can evaluate the predicted HOMO-LUMO gaps with the ground truth values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = PCQM4Mv2Evaluator()\n",
    "formatted_predictions = format_predictions(\n",
    "    dataset_name=cfg.dataset.dataset_name, y_true=ground_truth, y_pred=predictions\n",
    ")\n",
    "# we will use the official evaluator from the OGB repo\n",
    "result = evaluator.eval(formatted_predictions)[\"mae\"]\n",
    "print(\n",
    "    \" \" + \"=\" * 50 + \"\\n\",\n",
    "    f\"\\U00002B50 Result: Validation MAE = {result:.4f}\\n\",\n",
    "    \"=\" * 50 + \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the MAE is still relatively high as this example only shows training for 10 epochs.\n",
    "\n",
    "The small GPS++ 11M parameter model trained for the full 100 epochs should reach an MAE of ~ 0.090 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow up tasks\n",
    "\n",
    "Some additional tasks to explore include:\n",
    "* Try increasing the number of epochs to train the model over to achieve a better final validation MAE\n",
    "* Try the inference notebook if you haven't already\n",
    "* Read the paper on GPS++ for further details about the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import ipu\n",
    "\n",
    "ipu.config.reset_ipu_configuration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.2.0+1262_tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7a9523ff6ed7b4be1e3cbb6e3c37c88f3139ed0d5859d40d308cef69f5b32c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
